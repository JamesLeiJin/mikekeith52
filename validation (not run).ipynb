{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdaa0263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://jinlei:****@jfrog.ngridtools.com/artifactory/api/pypi/pypi-remote/simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement diagram_creator (from versions: none)\n",
      "ERROR: No matching distribution found for diagram_creator\n"
     ]
    }
   ],
   "source": [
    "! pip install diagram_creator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ae771-52a5-4084-80b5-340d5dbe41f5",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "There are many ways to validate a model with scalecast and this notebook introduces them and overviews the differences between dynamic and non-dynamic tuning/testing, cross-validation, backtesting, and the eye test.\n",
    "\n",
    "- Download data: https://www.kaggle.com/robervalt/sunspots  \n",
    "- See here for EDA on this dataset: https://scalecast-examples.readthedocs.io/en/latest/rnn/rnn.html  \n",
    "- See here for documentation on cross validation: https://scalecast.readthedocs.io/en/latest/Forecaster/Forecaster.html#src.scalecast.Forecaster.Forecaster.cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a535f3-6f6c-4d1e-833a-f84eda12437e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diagram_creator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscalecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mForecaster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Forecaster\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdiagram_creator\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diagram_creator'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scalecast.Forecaster import Forecaster\n",
    "import diagram_creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934672f-949c-49e6-838f-9c951d0ac7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fcst(f, test_length=0.1, fcst_length=120):\n",
    "    \"\"\" adds all variables and sets the test length/forecast length in the object\n",
    "    \n",
    "    Args:\n",
    "        f (Forecaster): the Forecaster object.\n",
    "        test_length (int or float): the test length as a size or proportion.\n",
    "        fcst_length (int): the forecast horizon.\n",
    "        \n",
    "    Returns:\n",
    "        (Forecaster) the processed object.\n",
    "    \"\"\"\n",
    "    f.generate_future_dates(fcst_length)\n",
    "    f.set_test_length(test_length)\n",
    "    f.set_validation_length(f.test_length)\n",
    "    f.eval_cis()\n",
    "    f.add_seasonal_regressors(\"month\")\n",
    "    for i in np.arange(60, 289, 12):  # 12-month cycles from 12 to 288 months\n",
    "        f.add_cycle(i)\n",
    "    f.add_ar_terms(120)  # AR 1-120\n",
    "    f.add_AR_terms((20, 12))  # seasonal AR up to 20 years, spaced one year apart\n",
    "    f.add_seasonal_regressors(\"year\")\n",
    "    #f.auto_Xvar_select(irr_cycles=[120],estimator='gbt')\n",
    "    return f\n",
    "\n",
    "\n",
    "def export_results(f):\n",
    "    \"\"\" returns a dataframe with all model results given a Forecaster object.\n",
    "    \n",
    "    Args:\n",
    "        f (Forecaster): the Forecaster object.\n",
    "        \n",
    "    Returns:\n",
    "        (DataFrame) the dataframe with the pertinent results.\n",
    "    \"\"\"\n",
    "    results = f.export(\"model_summaries\", determine_best_by=\"TestSetMAE\")\n",
    "    return results[\n",
    "        [\n",
    "            \"ModelNickname\",\n",
    "            \"TestSetRMSE\",\n",
    "            \"InSampleRMSE\",\n",
    "            \"ValidationMetric\",\n",
    "            \"ValidationMetricValue\",\n",
    "            \"HyperParams\",\n",
    "            \"TestSetLength\",\n",
    "            \"DynamicallyTested\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c09141-dbc1-45f3-8df0-f7ae8d2874ac",
   "metadata": {},
   "source": [
    "## Load Forecaster Object\n",
    "- we choose 120 periods (10 years) for all validation and forecasting\n",
    "- 10 years of observervations to tune model hyperparameters, 10 years to test, and a forecast horizon of 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e916c89-6dff-4131-b717-29573153f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\jinlei\\Documents\\2023\\mikekeith52\\data\\Sunspots.csv', index_col=0, names=[\"Date\", \"Target\"], header=0)\n",
    "f = Forecaster(y=df[\"Target\"], current_dates=df[\"Date\"])\n",
    "prepare_fcst(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9fe9f-3a48-4088-be91-370ed91ed88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.set_estimator('gbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b253b-7b6a-4b7c-9070-fe2e3f83c707",
   "metadata": {},
   "source": [
    "In the [feature_selection](https://scalecast-examples.readthedocs.io/en/latest/misc/feature-selection/feature_selection.html) notebook, gbt was chosen as the best model class out of several tried. We will show all examples with this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236691d-563e-442b-b14d-3c947f8d3993",
   "metadata": {},
   "source": [
    "## Default Model Parameters\n",
    "- one with dynamic testing\n",
    "- one with non-dynamic testing\n",
    "- the difference can be expressed by taking the case of a simple autoregressive model, such that:\n",
    "\n",
    "\n",
    "### Non-Dynamic Autoregressive Predictions\n",
    "\n",
    "$$\n",
    "x_t = \\alpha * x_{t-1} + e_t\n",
    "$$\n",
    "\n",
    "Over an indefinite forecast horizon, the above equation would only work if you knew the value for $x_{t-1}$. Going more than one period into the future, you would stop knowing what that value is. In a test-set of data, of course, you do know all values into the forecast horizon, but to be more realistic, you could write an equation for a two-step forecast like this:\n",
    "\n",
    "### Dynamic Autoregressive Predictions\n",
    "\n",
    "$$\n",
    "\\hat{x_t} = \\hat{\\alpha} * x_{t-1}\n",
    "$$\n",
    "$$\n",
    "x_{t+1} = \\hat{\\alpha} * \\hat{x}_t + e_{t+1}\n",
    "$$\n",
    "\n",
    "Using these two equations, which scalecast refers to as dynamic forecasting, you could evaluate any forecast horizon by plugging in predicted values for $x_{t-1}$ or ${x_t}$ over periods in which you did not know it. This is default behavior for all models tested through scalecast, but it is not default for tuning models. We will explore dynamic tuning soon. First, let's see in practical terms the difference between non-dynamic and dynamic testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e6396-96b7-4297-b8e6-3cdf9f2d908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.manual_forecast(call_me=\"gbt_default_non-dynamic\", dynamic_testing=False)\n",
    "f.manual_forecast(call_me=\"gbt_default_dynamic\")  # default is dynamic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990aacf-f999-42b0-a5c4-ca4c2b0ff3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.plot_test_set(\n",
    "    models=[\n",
    "        \"gbt_default_non-dynamic\", \n",
    "        \"gbt_default_dynamic\"\n",
    "    ], \n",
    "    include_train=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59058b00-608a-4797-887d-c433df14d9c1",
   "metadata": {},
   "source": [
    "It appears that the non-dynamically tested model performed significantly better than the other, but looks can be deceiving. In essence, the non-dynamic model was only tested for its ability to perform 326 one-step forecasts and its final metric is an average of these one-step forecasts. It could be a good idea to set `dynamic_testing=False` if you want to speed up the testing process or if you only care about how your model would perform one step into the future. But to report the test-set metric from this model as if it could be expected to do that well for the full 326 periods into the future is misleading. The other model that was dynamically tested can be more realistically trusted in that regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91600ea6-a5fb-43c3-b6b1-91d53ec62f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_results(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d9ad7-10c8-43e4-a647-41195496d630",
   "metadata": {},
   "source": [
    "## Tune the model to find optimal hyperparameters\n",
    "- Create a validation grid\n",
    "- Try three strategies to tune the parameters:\n",
    "  - Train/validation/test split\n",
    "    - Hyperparameters are tried on the validation set\n",
    "  - Train/test split with 5-fold time-series cross-validation on training set\n",
    "    - Training data split 5 times into train/validations set\n",
    "    - Models trained on training set only\n",
    "    - Validated out-of-sample\n",
    "    - All data available before each validation split sequentially used to train the model\n",
    "  - Train/test split with 5-fold time-series rolling cross-validation on training set\n",
    "    - Rolling is different in that each train/validation split is the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc06c6c-dc6a-4d7d-9d48-6b6c52fb6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"max_depth\": [2, 3, 5],\n",
    "    \"max_features\": [\"sqrt\", \"auto\"],\n",
    "    \"subsample\": [0.8, 0.9, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e074ba8-f94f-4c64-96bd-7b3eae352490",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.ingest_grid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1395398-3c73-4a18-a78c-03131fa3c013",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split\n",
    "- The data's sequence is always maintained in time-series splits with scalecast\n",
    "\n",
    "![](./mermaid-diagram-20220614194220.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99392428-d124-4a4d-8732-b309b2af56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.tune(dynamic_tuning=True)\n",
    "f.auto_forecast(\n",
    "    call_me=\"gbt_tuned\"\n",
    ")  # automatically uses optimal paramaeters suggested from the tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29c76d-45d4-4778-a14b-7bc32566256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.export_validation_grid(\"gbt_tuned\").sort_values('AverageMetric').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7b50b-662f-4aa0-a199-3c97e4db0fa2",
   "metadata": {},
   "source": [
    "### 5-Fold Time Series Cross Validation\n",
    "- Split training set into k (5) folds\n",
    "- Each validation set is the same size and determined such that: `val_size = n_obs // (folds + 1)`\n",
    "  - The last training set will be the same size or almost the same size as the validation sets\n",
    "- Model trained and re-trained with all data that came before each validation slice\n",
    "- Each fold tested out of sample on its validation set\n",
    "- Final error is an average of the out-of-sample error obtained from each fold\n",
    "- The chosen hyperparameters are determined by which final error was minimized\n",
    "- Below is an example with a dataset sized 100 observations and in which 10 observations are held out for testing and the remaining 90 observations are used as the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b099254-dc57-4454-ba51-f34c8e828b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_creator.create_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63fae8-61ce-4eb2-9d48-59ea00f283b9",
   "metadata": {},
   "source": [
    "The final error, *E*, can be expressed as an average of the error from each fold *i*: \n",
    "$$\n",
    "E = \\frac{1}{n}\\sum_{i=0}^{n-1}{(e_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82d746-a364-4182-b377-4526a5cfe236",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.cross_validate(\n",
    "    k=5, \n",
    "    dynamic_tuning=True,\n",
    "    test_length = None, # default so that last test and train sets are same size (or close to the same)\n",
    "    train_length = None, # default uses all observations before each test set\n",
    "    space_between_sets = None, # default adds a length equal to the test set between consecutive sets\n",
    "    verbose = True, # print out info about each fold\n",
    ")\n",
    "f.auto_forecast(call_me=\"gbt_cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04931fa-a8f8-445f-a7fa-ea191ed3faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.export_validation_grid(\"gbt_cv\").sort_values(\"AverageMetric\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee36e6e-6db2-4200-814f-c18f4dc1c956",
   "metadata": {},
   "source": [
    "### 5-Fold Rolling Time Series Cross Validation\n",
    "- Split training set into k (5) folds\n",
    "- Each validation set is the same size\n",
    "- Each training set is also the same size as each validation set\n",
    "- Each fold tested out of sample\n",
    "- Final error is an average of the out-of-sample error obtained from each folds\n",
    "- The chosen hyperparameters are determined by which final error was minimized\n",
    "- Below is an example with a dataset sized 100 observation and in which 10 observations are held out for testing and the remaining 90 observations are used as the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc073a1-c8f7-4eb0-b426-9542a01c633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_creator.create_rolling_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76929f8-ce14-40be-94ea-b700ff197958",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.cross_validate(\n",
    "    k=5, \n",
    "    rolling=True, \n",
    "    dynamic_tuning=True,\n",
    "    test_length = None, # with rolling = True, makes all train and test sets the same size\n",
    "    train_length = None, # with rolling = True, makes all train and test sets the same size\n",
    "    space_between_sets = None, # default adds a length equal to the test set between consecutive sets\n",
    "    verbose = True, # print out info about each fold\n",
    ")\n",
    "f.auto_forecast(call_me=\"gbt_rolling_cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6864ad4-f636-4c6b-a618-7886dfeb3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.export_validation_grid(\"gbt_rolling_cv\").sort_values(\"AverageMetric\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd90f8-2a68-4361-99f6-274d26ac6a6a",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0a64e-d863-45c5-a5be-9fab03da4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.plot_test_set(\n",
    "    models=[\n",
    "        \"gbt_default_dynamic\",\n",
    "        \"gbt_tuned\", \n",
    "        \"gbt_cv\", \n",
    "        \"gbt_rolling_cv\",\n",
    "    ], \n",
    "    include_train=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb8da2-ec50-4e6b-b49d-d73115eb168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 60)\n",
    "export_results(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20235fe-0964-4850-ae29-516adde28ceb",
   "metadata": {},
   "source": [
    "## Backtest models\n",
    "- Backtesting is a process in which the final chosen model is re-validated by seeing its average performance on the last x-number of forecast horizons available in the data\n",
    "- With scalecast, pipeline objects can be built and backtest all applied models to see the best one over several forecast horizons\n",
    "- See the [documentation](https://scalecast.readthedocs.io/en/latest/Forecaster/Pipeline.html#src.scalecast.Pipeline.Pipeline.backtest) for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27670d0-93ea-4450-a7a3-4cf7663c6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scalecast.Pipeline import Pipeline\n",
    "from scalecast.util import backtest_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745e1b7-20e3-4e1a-9146-3f5aef33b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecaster(f):\n",
    "    f.set_estimator('gbt')\n",
    "    f.set_validation_length(int(len(f.y)*.1)) # 10% val length each time\n",
    "    f.add_seasonal_regressors(\"month\")\n",
    "    for i in np.arange(60, 289, 12):  # 12-month cycles from 12 to 288 months\n",
    "        f.add_cycle(i)\n",
    "    f.add_ar_terms(120)  # AR 1-120\n",
    "    f.add_AR_terms((20, 12))  # seasonal AR up to 20 years, spaced one year apart\n",
    "    f.add_seasonal_regressors(\"year\")\n",
    "    \n",
    "    f.manual_forecast(call_me='gbt_default_dynamic')\n",
    "    \n",
    "    f.ingest_grid(grid)\n",
    "    \n",
    "    f.tune(dynamic_tuning = True)\n",
    "    f.auto_forecast(call_me='gbt_tuned')\n",
    "    \n",
    "    f.cross_validate(dynamic_tuning = True)\n",
    "    f.auto_forecast(call_me='gbt_cv')\n",
    "    \n",
    "    f.cross_validate(dynamic_tuning = True, rolling = True)\n",
    "    f.auto_forecast(call_me='gbt_rolling_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fb94f-c697-49bd-a70a-4de6a4b9b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('Forecast',forecaster)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ce82e-27a6-4841-9d5c-edf1566fb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results = pipeline.backtest(\n",
    "    f,\n",
    "    test_length = 0,\n",
    "    fcst_length = 120,\n",
    "    jump_back = 120, # place 120 obs between each training set\n",
    "    cis = False,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4e25e-81d3-4513-a837-3258e4d9e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = backtest_metrics(backtest_results,mets=['rmse','mae','r2'])\n",
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb79ff1-6c04-47cb-b5c5-d834a9d7459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmr = bm.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2450df-5ef2-424f-9f57-661baf60b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmrrmse = bmr.loc[bmr['Metric'] == 'rmse'].sort_values('Average')\n",
    "best_model = bmrrmse.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3235acf-be32-4dac-8cf2-ae2601c99dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = bmr.loc[\n",
    "    (bmr['Model'] == best_model) & (bmr['Metric'] == 'rmse'),\n",
    "    'Average',\n",
    "].values[0]\n",
    "mae = bmr.loc[\n",
    "    (bmr['Model'] == best_model) & (bmr['Metric'] == 'mae'),\n",
    "    'Average',\n",
    "].values[0]\n",
    "r2 = bmr.loc[\n",
    "    (bmr['Model'] == best_model) & (bmr['Metric'] == 'r2'),\n",
    "    'Average',\n",
    "].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370b531-64dc-4c8a-980c-c8c1c49239dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The best model, according to the RMSE over 5 backtest iterations was {best_model}.\"\n",
    "    \" On average, we can expect this model to have an RMSE of\"\n",
    "    f\" {rmse:.2f}, MAE of {mae:.2f}, and R2 of {r2:.2%} over a full 120-period\"\n",
    "    \" forecast window.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329efb64-1633-46bf-bc3b-4ce24c5686a6",
   "metadata": {},
   "source": [
    "An extension of this analysis could be to choose regressors more carefully (see [here](https://scalecast-examples.readthedocs.io/en/latest/misc/feature-selection/feature_selection.html)) and to use more complex models (see [here](https://scalecast-examples.readthedocs.io/en/latest/rnn/rnn.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa913a-5a13-48fe-a4e8-13e86e4c7a45",
   "metadata": {},
   "source": [
    "## The Eye Test\n",
    "\n",
    "In addition to all the objective validation performed in this notebook, one of the most important questions to ask is if the forecast looks reasonable. Does it pass the common sense test? Below, we plot the 120-forecast period horizon from the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404a888-02bf-4c68-a2ea-ee68dcb47710",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.plot(models=best_model, ci = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e7cb2-7a3e-41e4-90e2-496f368d5846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
